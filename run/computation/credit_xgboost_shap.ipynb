{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Credit â€” XGBoost SHAP Computation\n",
    "\n",
    "This notebook trains a XGBoost classifier on the **German Credit** dataset and computes SHAP values and SHAP interaction values for model explainability. The results are saved to disk for downstream visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Workspace\\IJAR\\IJAR-python\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shap\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Fetch the German Credit dataset from OpenML. The target variable is binarized as `1` (good credit) vs `0` (bad credit). Categorical features are one-hot encoded. The feature matrix is persisted as a pickle for reuse in visualization notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = fetch_openml(name=\"credit-g\", version=1, as_frame=True)\n",
    "X = credit.data\n",
    "y = (credit.target == \"good\").astype(int)\n",
    "X = pd.get_dummies(X, drop_first=True).astype(float)\n",
    "\n",
    "# Save original feature names\n",
    "os.makedirs(\"../../data/credit/xgboost\", exist_ok=True)\n",
    "X.to_pickle(\"../../data/credit/x_values.pkl\")\n",
    "y.to_pickle(\"../../data/credit/y_values.pkl\")\n",
    "\n",
    "# Clean feature names for XGBoost/SHAP (remove problematic characters)\n",
    "X.columns = X.columns.str.replace(r'[<>\\[\\]]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "Split the data into 80% training and 20% test sets and wrap them in `xgboost.DMatrix` objects required by the XGBoost API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_full = xgboost.DMatrix(X, label=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "xgb_train = xgboost.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgboost.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the XGBoost survival model\n",
    "\n",
    "Train an XGBoost model with the Cox proportional-hazards objective (`survival:cox`) on the **full** dataset for 5 000 boosting rounds with a low learning rate (0.002) and 50% row subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-cox-nloglik:6.55108\n",
      "[1000]\ttest-cox-nloglik:6.56052\n",
      "[2000]\ttest-cox-nloglik:6.57230\n",
      "[3000]\ttest-cox-nloglik:6.58401\n",
      "[4000]\ttest-cox-nloglik:6.59267\n",
      "[4999]\ttest-cox-nloglik:6.60038\n"
     ]
    }
   ],
   "source": [
    "params = {\"eta\": 0.002, \"max_depth\": 3, \"objective\": \"survival:cox\", \"subsample\": 0.5}\n",
    "model = xgboost.train(params, xgb_full, 5000, evals=[(xgb_full, \"test\")], verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SHAP values\n",
    "\n",
    "Use `shap.TreeExplainer` to compute SHAP values for the first 500 samples. For a classifier the explainer returns per-class values; we extract and save only the **positive class** (good credit) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500\n",
    "X_shapley = X.iloc[:num_samples, :]\n",
    "explainer = shap.TreeExplainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_shapley)\n",
    "np.save(\"../../data/credit/xgboost/shap_values.npy\", shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SHAP interaction values\n",
    "\n",
    "Compute pairwise SHAP interaction values for the same 500 samples. These capture feature-pair synergies and redundancies and are saved for network-based visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_interaction_values = explainer.shap_interaction_values(X_shapley)\n",
    "np.save(\"../../data/credit/xgboost/shap_interaction_values.npy\", shap_interaction_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
